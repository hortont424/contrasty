\documentclass{acmsiggraph}

\usepackage{parskip}
\usepackage{graphicx}
\usepackage{courier}
\usepackage{footmisc}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{url}
\usepackage{color}
\usepackage{xcolor}
\onlineid{0}

\title{Deriving Depth From a Fixed-Position Variable-Focus Camera}

\author{Tim Horton\thanks{e-mail: hortot2@rpi.edu}\\Rensselaer Polytechnic Institute}

\begin{document}

\maketitle

\section*{Abstract}

Much work has been done regarding the capture of three-dimensional data from the real world into a computer. Some approaches have multiple cameras capturing from different perspectives; others use a grid of known geometry projected onto the scene and measure its deformation.

This paper will explore the possibility of capturing three-dimensional data using a single camera, at a fixed point in space, and without manipulating the scene with projected light or otherwise. Instead, we will focus on the image processing technique required to extract depth information from multiple frames taken with a shallow depth-of-field at different focal distances.

\section{Introduction}

The human brain generally uses perspective information gleaned from having two sensors --- our eyes --- placed at different points in space in order to determine the depth of objects in a scene. However, while the loss of one eye does significantly hinder one's ability to capture depth information, the brain is still able to capture \emph{rough} depth from other sources.

One such source is the focal blur caused by the fact that the human eye is not a pinhole camera --- it does not have an infinitely small aperture. The brain can separate a scene into layers of depth based on where in its focal range each part of the scene comes into focus. It's this biological approach which we will attempt to reimplement as a computer vision algorithm.

\section{Prior Art}

(Adelson, 1992) presents a similar system, however they make use of a specialized plenoptic filter which allows them to capture multiple focal distances in one frame, and they utilize an offset aperture, using the displacement of out-of-focus image elements to determine depth, instead of contrast as used in this paper. Their method is much more accurate but, unlike ours, requires specialized equipment.

\section{Data}

The sample data used for evaluation and development of this algorithm consists of a set of series of images. Each series is of a fixed scene, and each image within the series is taken at a different focal distance, from closest to the camera towards infinity.

Two scenes in particular were used for the majority of the development --- one complicated scene, with many objects of varying sizes and depths, and one simple scene (Figures \ref{fig:4-2} and \ref{fig:4-6} are three sample frames from this set), with three flat objects of similar size, placed parallel to the plane of focus, and taking up relatively equal, large portions of the frame.

All images were captured with Nikon's 50mm f/1.8D lens, giving a depth-of-field of a few centimeters, sufficiently small enough to separate a few different layers of depth.

\begin{figure}
\includegraphics[width=84.5mm]{4-2.JPG}
\caption{Frame from simple scene (4:2, focus=59cm)}
\label{fig:4-2}
\end{figure}

\begin{figure}
\includegraphics[width=84.5mm]{4-6.JPG}
\caption{Frame from simple scene (4:6, focus=84cm)}
\label{fig:4-6}
\end{figure}

\section{Algorithm}

\begin{figure*}
\includegraphics[width=169mm]{flowcharts.eps}
\caption{Algorithm Overview}
\label{fig:algorithm-overview}
\end{figure*}

\subsection{Breathing Correction}

The first step in our algorithm involves correcting for an imperfection in the design of the lens used to capture these images, where the focal length of the lens changes slightly with the focal distance. This results in changes in the framing of the resulting image as the point of focus is passed through the scene. Most photographic lenses have this imperfection, to varying degrees, requiring calibration for every lens (cinematic lenses, on the other hand, often internally correct for breathing, though at great monetary cost).

\begin{figure}
\includegraphics[width=84.5mm]{focusbreathing.eps}
\caption{Framing at both ends of focus, demonstrating breathing (to scale)}
\label{fig:focusbreathing}
\end{figure}

To correct for breathing, we take sample images at many focal distances of a point source of light near one of the corners of the image. The images are then thresholded to create a series of binary images each with a single feature: a circle of varying size, centered directly on the light source. Computing the center of mass of this feature gives us the location of the light. The ratios of distances between the light and the center of each image gives a scale factor, which can then have a curve fit to it (scale factor vs. focal distance).

For the lens used in this project, the scale factor is directly proportional to focal distance --- with ten samples, \[scale=(0.0539 \cdot d_{focus}) + 0.998\]

Once the scale curve has been determined, it is applied to each set of images, and all images are cropped down to the size of the smallest in the set.

\subsection{Contrast Detection}

Since determining the depth at which different regions of the image are in focus is crucial to this algorithm, we need a way to measure how "in-focus" a region is. It seems reasonable to look to camera manufacturers here, since they have a great deal of experience in constructing autofocus mechanisms, which must also have some measure of focus. Unfortunately the fast and accurate autofocus mechanism used by most modern DSLRs is impossible to implement after the photograph has been taken, as it depends on phase detection of two different images. However, autofocus in "live view" on a DSLR or on cheaper point-and-shoot cameras uses a simpler method, which only requires inspection of the image itself: contrast detection.

\begin{figure}
\includegraphics[width=84.5mm]{4-2-C.JPG}
\caption{Contrast-detected frame from simple scene (4:2, focus=59cm)}
\label{fig:4-2-C}
\end{figure}

\begin{figure}
\includegraphics[width=84.5mm]{4-6-C.JPG}
\caption{Contrast-detected frame from simple scene (4:6, focus=70cm)}
\label{fig:4-6-C}
\end{figure}

\begin{figure}
\includegraphics[width=84.5mm]{4-2-C-MAX.JPG}
\caption{Max-filtered contrast-detected frame from simple scene (4:2, focus=59cm)}
\label{fig:4-2-C-MAX}
\end{figure}

\begin{algorithm}
\caption{Contrast Detection Kernel}
\label{alg:contrast-kernel}
\begin{algorithmic}
\STATE $s \gets 36 / (1 + 2 \cdot r)^2$

\FOR{$ix = -r$ \TO $r$}
\FOR{$iy = -r$ \TO $r$}
\STATE $gauss \gets exp(0.5 \cdot s \cdot ((ix - x)^2 + (iy - y)^2))$
\STATE $val \gets val + abs(img_{in}[x, y] - img_{in}[ix, iy]) * gauss$
\STATE $sum \gets sum + gauss$
\ENDFOR
\ENDFOR

\STATE $img_{out}[x, y] \gets val / sum$
\end{algorithmic}
\end{algorithm}

\begin{figure}
\includegraphics[width=84.5mm]{4-REDUCED.JPG}
\caption{Reduced preliminary depth frame from simple scene (4)}
\label{fig:4-REDUCED}
\end{figure}

Contrast detection works as a measure of focus simply because of how lens blur works: in out-of-focus regions, the size of the circle of confusion is very large, and the summing of these circles causes a smooth blur, decreasing the local contrast --- alternatively, when in-focus, the circle of confusion is very small (i.e. the image is "sharp"), and the contrast from the scene is retained.

This method of contrast detection does, however, depend on there being contrast in the source scene at all. Section \ref{sec:problems-flat} will discuss this limitation in more detail.

In our contrast detection algorithm, each pixel is replaced by the weighted average of the difference between that pixel and all of the pixels in its neighborhood, as can be seen in Algorithm \ref{alg:contrast-kernel}. The resulting images have relatively bright regions in areas of high contrast, and vice versa, as can be seen in Figures \ref{fig:4-2-C} and \ref{fig:4-6-C}.

After the contrast detection step, a large local maximum filter is applied to the image, resulting in Figure \ref{fig:4-2-C-MAX}. This helps to make regions of low contrast surrounded by regions of high contrast take the high contrast value, correcting a fundamental problem with the algorithm which will be discussed in \ref{sec:problems-flat}. However, it also destroys any legitimate fine detail in the depth information up to the size of the filter.

\subsection{Merging and Reduction}

We now have one contrast-filtered layer for each input image; the next step is to determine the depth of each pixel. To do this, we stack the layers up, and create a new image which has the index of the brightest layer for each pixel. If a pixel appears never to come into focus (it remains approximately constant or 0 throughout all of the layers), it is discarded (given an invalid depth). The result of this step can be seen in Figure \ref{fig:4-REDUCED}, and closely resembles the final product.

\subsection{Filling}

Some pixels were given an invalid depth during the reduction step, due to the fact that they seemed not to come to focus; as one final step, we fill in any holes. Filling is performed by voting --- at each "dead" pixel --- into 256 bins (one for each greyscale value) based on the value of all of the pixels in some neighborhood, and then replacing the value of the current pixel with the most-occurring other color.

The filling process is repeated continuously until no invalid pixels remain.

\section{Visualization}

While extracting depth information by itself is interesting, outputting the depth data in a more interesting and human-parseable format seemed to be a better goal.

I had initially planned to implement a few visualization methods which I later deemed too difficult with the given data, since our algorithm only gives as many levels of depth as there are input frames. With a 3cm depth-of-field, there simply aren't enough levels to implement anything depending on smooth depth information. A potential fix to this is discussed in Section \ref{sec:fitting-focus}, but was not implemented in time for this paper.

\subsection{Infinite Depth-of-Field}

\label{sec:infinite-dof}

Since we have both depth information as well as the original frames from which this information was gleaned, we can use the depth map to selectively mask all of the in-focus regions, resulting in a completely-in-focus image, as if it had been taken with a pinhole camera.

You can see the results of this visualization in Figures \ref{fig:4-INF} and \ref{fig:1-INF}. Notice especially the noise in the low-contrast background in Figure \ref{fig:1-INF}, which will be discussed in section \ref{sec:problems-flat}, but also that the pictures are --- in general --- in focus throughout the frame.

\subsection{Anaglyph 3D}

\begin{figure}[t]
\includegraphics[width=84.5mm]{4-INF.JPG}
\caption{Infinite focus output from simple scene (4)}
\label{fig:4-INF}
\end{figure}

\begin{figure}[t]
\includegraphics[width=84.5mm]{1-INF.JPG}
\caption{Infinite focus output from complex scene (1)}
\label{fig:1-INF}
\end{figure}

\begin{figure}[t]
\includegraphics[width=84.5mm]{1-ANA.PNG}
\caption{Anaglyph output from complex scene (1)}
\label{fig:1-ANA}
\end{figure}

The next obvious visualization method is to map the image into a simulated three dimensional image. The easiest way to do this is anaglyph 3D, which is commonly used with red-cyan glasses to provide the 3D effect.

To create an anaglyph image, we use the normalized depth image (we output two different depth images: one where the grey value is equal to the frame at which that pixel came into focus, and one where the pixel values range from 0 to 255, normalized by increasing depth) as a displacement map onto the separate color channels of the "infinite DOF" (from Section \ref{sec:infinite-dof}) image. The red channel is shifted to the left by the depth, the blue and green channels are shifted to the right, resulting in an image similar to that seen in Figure \ref{fig:1-ANA}.

Viewed through red-cyan glasses, one can certainly see the layers of depth arranged as they were in the original scene. Since there are only a few discrete layers, the image does not seem "realistic" by any means\footnote{The fact that it's greyscale doesn't help that fact at all}, but it provides an interesting visualization of the output of our approach.

\section{Implementation}

The majority of the skeleton of our program is implemented in Python\footnote{\url{http://www.python.org}}. However, performance-critical portions (most of the filtering operations) are implemented in OpenCL\footnote{\url{http://www.khronos.org/opencl/}}, which runs in a massively-parallel fashion at C-like speeds on either the CPU or GPU. The Python Imaging Library\footnote{\url{http://www.pythonware.com/products/pil/}} is used for image I/O (and, during development, rapid display of images); Andreas Kl\"{o}ckner's PyOpenCL\footnote{\url{http://mathema.tician.de/software/pyopencl}} is used to wrap the OpenCL API; numpy and scipy\footnote{\url{http://scipy.org/}} are used for some filtering and analysis operations, and for interfacing between PIL and PyOpenCL; PyOpenGL\footnote{\url{http://pyopengl.sourceforge.net/}} is used for an unfinished OpenGL-based depth viewer; and Phil Harvey's exiftool\footnote{\url{http://www.sno.phy.queensu.ca/~phil/exiftool/}} is used to extract focal distance metadata for the breathing correction phase.

The code is all available (under the 2-clause BSD license) at \url{http://github.com/hortont424/contrasty}

\subsection{Memory Limitations}

While the kernels are implemented in OpenCL, and should theoretically be capable of running on a GPU (providing a large speedup, as image filtering is what GPUs are good at!), the merging and reduction kernels, as they're implemented, require the ability to operate on incredibly large textures --- upwards of 400MB in the case of 11$\times$16 megapixel layers. Finding a video card with that kind of \emph{available} memory isn't something I'm going to do for this project, so I've just been running it on the CPU.

This, of course, means that turnaround times for the entire process are on the order of 450 seconds, so it takes quite a while to develop the algorithm; testing a small change takes quite a while. For part of the development time, significantly scaled images were used, but this adversely affected the accuracy of contrast detection, and all of the resolution-dependent filters were thrown off.

\section{Results}

\subsection{Simple Scene}

\subsection{Complicated Scene}

\subsection{Analysis \& Known Problems}

\subsubsection{Analysis}

\subsubsection{Limited Range of Depths}

\subsubsection{Breathing Correction}

also talk about focus exif data being inaccurate

\subsubsection{Areas of Flat Color}

\label{sec:problems-flat}

\subsubsection{Bright Points}

\subsubsection{Color Contrast}

\section{Applications}

\section{Future Work}

\subsection{Using the EXIF for Depth}

\subsection{Optimization of Results}

\subsection{Performance Optimization}

\subsection{Fitting the Focus}

\label{sec:fitting-focus}

\subsection{Visualization}

\section{Conclusion}

\section{References}

$http://www.ncbi.nlm.nih.gov/pubmed/8728981
http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=121783$

\end{document}